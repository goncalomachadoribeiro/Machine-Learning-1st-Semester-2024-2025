{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.applications import ResNet50, VGG19, VGG16\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "X_labeled = np.load(\"Xtrain1.npy\")\n",
    "X_unlabeled = np.load(\"Xtrain1_extra.npy\")\n",
    "Y_train_labeled = np.load(\"Ytrain1.npy\")\n",
    "X_test = np.load(\"Xtest1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2783, 2304)\n",
      "(904, 2304)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the shape of the data\n",
    "\n",
    "print(X_labeled.shape)\n",
    "print(X_unlabeled.shape)\n",
    "\n",
    "# Normalization\n",
    "\n",
    "X_labeled = X_labeled/255\n",
    "X_unlabeled = X_unlabeled/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# Turning the data into 2D\n",
    "\n",
    "resized = np.resize(X_labeled[1], [48,48])\n",
    "plt.imshow(resized, cmap='gray')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Imbalance\n",
    "\n",
    "crater_images = sum(Y_train_labeled)\n",
    "no_crater_images = len(Y_train_labeled) - crater_images\n",
    "ratio_no_crater_crater = no_crater_images/crater_images\n",
    "print(\"Number of crater images: \", crater_images)\n",
    "print(\"Number of no crater images: \", no_crater_images)\n",
    "print(\"Ratio no crater/crater: \", ratio_no_crater_crater)\n",
    "\n",
    "# Class Weights\n",
    "\n",
    "class_weights_first = compute_class_weight(class_weight='balanced', classes=np.unique(Y_train_labeled), y=Y_train_labeled.flatten())\n",
    "class_weight_dict_first = dict(enumerate(class_weights_first))\n",
    "print(\"Class weights: \", class_weight_dict_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and Data Splitting\n",
    "\n",
    "X_train_reshaped = X_labeled.reshape(len(X_labeled), 48, 48, 1)\n",
    "X_unlabeled_train = X_unlabeled.reshape(len(X_unlabeled), 48, 48, 1)\n",
    "X_test_reshaped = X_test.reshape(len(X_test), 48, 48, 1)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_reshaped, Y_train_labeled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for Data Imbalance: Data Augmentation only on Minority Class\n",
    "\n",
    "Data augmentation is a technique that can be used to improve the robustness and accuracy of a machine learning model by creating additional examples from the original dataset.\n",
    "\n",
    "https://www.picsellia.com/post/improve-imbalanced-datasets-in-computer-vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of samples in each class\n",
    "\n",
    "majority_indices = np.where(Y_train == 1)[0]\n",
    "minority_indices = np.where(Y_train == 0)[0]\n",
    "\n",
    "X_train_majority = X_train[majority_indices]\n",
    "X_train_minority = X_train[minority_indices]\n",
    "\n",
    "target_minority_samples = len(X_train_majority)\n",
    "num_augmented_samples = target_minority_samples - len(X_train_minority)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "augmented_minority_images = []\n",
    "for i in range(num_augmented_samples):\n",
    "    img = X_train_minority[i % len(X_train_minority)].reshape((1, 48, 48, 1))\n",
    "    augmented_img = next(datagen.flow(img, batch_size=1))[0]\n",
    "    augmented_minority_images.append(augmented_img)\n",
    "\n",
    "X_train_minority_balanced = np.concatenate([X_train_minority, np.array(augmented_minority_images)])\n",
    "Y_train_minority_balanced = np.zeros(len(X_train_minority_balanced))\n",
    "\n",
    "X_train_balanced_DA = np.concatenate([X_train_majority, X_train_minority_balanced])\n",
    "Y_train_balanced_DA = np.concatenate([np.ones(len(X_train_majority)), Y_train_minority_balanced])\n",
    "\n",
    "indices = np.arange(len(Y_train_balanced_DA))\n",
    "np.random.shuffle(indices)\n",
    "X_train_balanced_DA, Y_train_balanced_DA = X_train_balanced_DA[indices], Y_train_balanced_DA[indices]\n",
    "\n",
    "print(\"Balanced training set shape:\", X_train_balanced_DA.shape)\n",
    "print(\"Balanced class distribution:\", np.bincount(Y_train_balanced_DA.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for Data Imbalance: Random Sampling\n",
    "\n",
    "https://medium.com/@shubhamgupta.3101994/addressing-data-imbalance-in-image-classification-techniques-and-strategies-b922fb3c5124\n",
    "https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "\n",
    "There are two main approaches to random resampling for imbalanced classification; they are oversampling and undersampling.\n",
    "\n",
    " - Random Oversampling: Randomly duplicate examples in the minority class.\n",
    " - Random Undersampling: Randomly delete examples in the majority class.\n",
    "\n",
    "Random resampling provides a naive technique for rebalancing the class distribution for an imbalanced dataset.\n",
    "Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\n",
    "\n",
    "Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Over Sampling (ROS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train_balanced_DA.reshape(len(X_train_balanced_DA), -1)\n",
    "ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_ROS, Y_train_ROS = ros.fit_resample(X_train_flattened, Y_train_balanced_DA)\n",
    "X_train_ROS = X_train_ROS.reshape(-1, 48, 48, 1)\n",
    "unique, counts = np.unique(Y_train_ROS, return_counts=True)\n",
    "print(\"Balanced class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "print(X_train_ROS.shape)\n",
    "print(Y_train_ROS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Under Sampling (RUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "X_train_RUS, Y_train_RUS = rus.fit_resample(X_train_flattened, Y_train)\n",
    "X_train_RUS = X_train_RUS.reshape(-1, 48, 48, 1)\n",
    "\n",
    "unique, counts = np.unique(Y_train_RUS, return_counts=True)\n",
    "\n",
    "print(\"Balanced class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of ROS and RUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy=0.95, random_state=42)\n",
    "\n",
    "X_train_over, Y_train_over = ros.fit_resample(X_train_flattened, Y_train)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "\n",
    "X_train_both, Y_train_both = rus.fit_resample(X_train_over, Y_train_over)\n",
    "\n",
    "X_train_both = X_train_both.reshape(-1, 48, 48, 1)\n",
    "\n",
    "unique, counts = np.unique(Y_train_over, return_counts=True)\n",
    "\n",
    "print(\"Final balanced class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional Layer 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flattening\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layer with dropout\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer (binary classification)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8) # NAO MEXE, NAO MUDA, NAO RESPIRA\n",
    "    # Compile the model\n",
    "    model.compile(optimizer= adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # Metric to monitor\n",
    "    patience=5, # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(Y_train_both), y=Y_train_both)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_both, Y_train_both,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "val_preds = np.round(val_preds)\n",
    "print(val_preds)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(Y_val, val_preds))\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_val, val_preds, average = 'macro')\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_val, val_preds)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plotting training history\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rgb = np.repeat(X_train_both, 3, axis=-1)  # Converts (48, 48, 1) to (48, 48, 3)\n",
    "X_val_rgb = np.repeat(X_val, 3, axis=-1)\n",
    "\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "# Freeze the initial layers of VGG19 to retain pre-trained features\n",
    "for layer in base_model.layers[:-8]:  # Adjust number of layers to unfreeze more, if necessary\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers on top\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x= Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_rgb, Y_train_both, \n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_rgb, Y_val),\n",
    "    epochs=30,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "val_preds = model.predict(X_val_rgb)\n",
    "val_preds = np.round(val_preds)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(Y_val, val_preds))\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_val, val_preds, average='macro')\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_val, val_preds)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plotting training history\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "for layer in base_model.layers[:-8]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_rgb, Y_train_both,\n",
    "    batch_size=32, \n",
    "    validation_data=(X_val_rgb, Y_val),\n",
    "    epochs=30,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "val_preds = model.predict(X_val_rgb)\n",
    "val_preds = np.round(val_preds)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(Y_val, val_preds))\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_val, val_preds, average='macro')\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_val, val_preds)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plotting training history\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the unlabeled data (values between 0 and 1)\n",
    "X_unlabeled_norm = X_unlabeled\n",
    "X_unlabeled_norm = X_unlabeled_norm.reshape(X_unlabeled_norm.shape[0], 48, 48, 1)\n",
    "\n",
    "# Train the CNN model on labeled data first (the code you already have)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # Metric to monitor\n",
    "    patience=5, # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "# Train the model on the labeled dataset\n",
    "history = model.fit(\n",
    "    X_train_both, Y_train_both,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=epochs,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "# Use the trained CNN model to generate pseudo-labels for the unlabeled data\n",
    "# Get predictions for the unlabeled data\n",
    "unlabeled_preds = model.predict(X_unlabeled_norm)\n",
    "print(unlabeled_preds)\n",
    "\n",
    "# Threshold to assign pseudo-labels based on prediction confidence\n",
    "pseudo_labels = np.where(unlabeled_preds > 0.95, 1, np.where(unlabeled_preds < 0.05, 0, -1))\n",
    "\n",
    "# Only keep confident predictions (those with pseudo-labels not equal to -1)\n",
    "confident_indices = np.where(pseudo_labels != -1)[0]\n",
    "X_confident = X_unlabeled_norm[confident_indices]\n",
    "pseudo_labels_confident = pseudo_labels[confident_indices]\n",
    "\n",
    "# Reshape pseudo_labels_confident if needed\n",
    "pseudo_labels_confident = pseudo_labels_confident.ravel()\n",
    "\n",
    "# Combine the original labeled data with the pseudo-labeled confident data\n",
    "X_combined = np.concatenate((X_train, X_confident), axis=0)\n",
    "y_combined = np.concatenate((Y_train, pseudo_labels_confident), axis=0)\n",
    "\n",
    "print(X_combined.shape)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # Metric to monitor\n",
    "    patience=5, # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "\n",
    "# Retrain the model using the combined labeled + pseudo-labeled data\n",
    "model = create_model()  # Recreate the model to retrain\n",
    "history_combined = model.fit(\n",
    "    X_combined, y_combined,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=epochs,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "val_preds = model.predict(X_val)\n",
    "val_preds = np.round(val_preds)\n",
    "plt.hist(np.where((unlabeled_preds > 0.05) & (unlabeled_preds < 0.95)))\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(Y_val, val_preds))\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_val, val_preds, average='macro')\n",
    "print(\"F1 Score after self-training: \", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_val, val_preds)\n",
    "print(\"Confusion Matrix after self-training:\\n\", conf_matrix)\n",
    "\n",
    "# Step 6: Plotting training history for combined data\n",
    "plt.plot(history_combined.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history_combined.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy After Self-Training')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_combined.history['loss'], label='train loss')\n",
    "plt.plot(history_combined.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss After Self-Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "f1_scores = []\n",
    "histories = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_labeled):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "\n",
    "    # Ensure indices are within bounds\n",
    "    train_index = train_index[train_index < len(X_train_both)]\n",
    "    val_index = val_index[val_index < len(X_train_both)]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X_train_both[train_index], X_train_both[val_index]\n",
    "    y_train, y_val = Y_train_both[train_index], Y_train_both[val_index]\n",
    "\n",
    "    \n",
    "    # Create a new CNN model for each fold\n",
    "    model = create_model()\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    \n",
    "    patience=5, \n",
    "    restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # Train the model with data augmentation\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=32, \n",
    "        validation_data=(X_val.reshape(X_val.shape[0], 48, 48, 1), y_val),  # Ensure X_val has shape (48, 48, 1)\n",
    "        epochs=20,\n",
    "        callbacks = [early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Append the training history for later analysis\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_preds = model.predict(X_val.reshape(X_val.shape[0], 48, 48, 1))\n",
    "    val_preds = np.round(val_preds)  # Convert probabilities to binary 0 or 1\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    f1 = f1_score(y_val, val_preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1 Score for Fold {fold}: {f1}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "    print(f\"Confusion Matrix for Fold {fold}:\\n\", conf_matrix)\n",
    "\n",
    "    # Increment fold number\n",
    "    fold += 1\n",
    "\n",
    "# Print average F1 score\n",
    "print(f\"\\nAverage F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy')\n",
    "plt.title('Model Accuracy Across Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['loss'], label=f'Fold {i+1} Train Loss')\n",
    "    plt.plot(history['val_loss'], label=f'Fold {i+1} Validation Loss')\n",
    "plt.title('Model Loss Across Folds')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a CNN for feature extraction and training the model with a DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using reshaped training data (labeled)\n",
    "x_label = X_train_both  # original labeled data (training set)\n",
    "y_label = Y_train_both  # original labeled data (labels)\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "f1_scores = []\n",
    "histories = []\n",
    "\n",
    "# Build the CNN feature extractor model\n",
    "def create_cnn_feature_extractor():\n",
    "    inputs = Input(shape=(48, 48, 1))\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Flattening\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return Model(inputs, x)\n",
    "\n",
    "# Build the DNN model\n",
    "def create_dnn(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(x_label):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = x_label[train_index], x_label[val_index]\n",
    "    y_train, y_val = y_label[train_index], y_label[val_index]  # Corrected\n",
    "\n",
    "    # Convert labels to binary (0 or 1)\n",
    "    y_train_binary = np.round(y_train).astype(int)  # Binary labels\n",
    "    y_val_binary = np.round(y_val).astype(int)\n",
    "\n",
    "    # Shuffle the resampled training data\n",
    "    X_train, y_train_binary = shuffle(X_train, y_train_binary, random_state=42)\n",
    "    \n",
    "    # Reshape the validation data to match CNN input\n",
    "    X_train = X_train.reshape((X_train.shape[0], 48, 48, 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], 48, 48, 1))\n",
    "\n",
    "    # Create CNN feature extractor\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor()\n",
    "    cnn_features_train = cnn_feature_extractor.predict(X_train)\n",
    "    cnn_features_val = cnn_feature_extractor.predict(X_val)\n",
    "    \n",
    "    # Flatten the CNN features\n",
    "    cnn_features_train = cnn_features_train.reshape((cnn_features_train.shape[0], -1))  # Flatten the features\n",
    "    cnn_features_val = cnn_features_val.reshape((cnn_features_val.shape[0], -1))  # Flatten the features\n",
    "    \n",
    "    # Create the DNN model\n",
    "    dnn_model = create_dnn(cnn_features_train.shape[1:])\n",
    "    \n",
    "    # Compile the DNN model\n",
    "    dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',     # Metric to monitor\n",
    "        patience=5,             # Number of epochs to wait for improvement\n",
    "        restore_best_weights=True  # Restore weights from the best epoch\n",
    "    )\n",
    "\n",
    "    # Ensure that y_train_binary and y_val_binary are reshaped to (batch_size, 1)\n",
    "    y_train_binary = y_train_binary.reshape((-1, 1))\n",
    "    y_val_binary = y_val_binary.reshape((-1, 1))\n",
    "\n",
    "    print(\"Shape of y_train_binary after reshaping:\", y_train_binary.shape)\n",
    "    print(\"Shape of y_val_binary after reshaping:\", y_val_binary.shape)\n",
    "\n",
    "    # Train the DNN model\n",
    "    history = dnn_model.fit(\n",
    "        cnn_features_train, y_train_binary,\n",
    "        validation_data=(cnn_features_val, y_val_binary),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Append the training history for later analysis\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the DNN model\n",
    "    val_preds = dnn_model.predict(cnn_features_val)\n",
    "    val_preds = np.round(val_preds).astype(int)  # Convert probabilities to binary 0 or 1\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    f1 = f1_score(y_val_binary, val_preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1 Score for Fold {fold}: {f1}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val_binary, val_preds)\n",
    "    print(f\"Confusion Matrix for Fold {fold}:\\n\", conf_matrix)\n",
    "\n",
    "    # Increment fold number\n",
    "    fold += 1\n",
    "\n",
    "# Print average F1 score\n",
    "print(f\"\\nAverage F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy')\n",
    "plt.title('Model Accuracy Across Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['loss'], label=f'Fold {i+1} Train Loss')\n",
    "    plt.plot(history['val_loss'], label=f'Fold {i+1} Validation Loss')\n",
    "plt.title('Model Loss Across Folds')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN + DNN + Unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using reshaped training data (labeled)\n",
    "x_label = X_combined  # original labeled data (training set)\n",
    "y_label = y_combined  # original labeled data (labels)\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "f1_scores = []\n",
    "histories = []\n",
    "\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(x_label):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = x_label[train_index], x_label[val_index]\n",
    "    y_train, y_val = y_label[train_index], y_label[val_index]  # Corrected\n",
    "\n",
    "    # Convert labels to binary (0 or 1)\n",
    "    y_train_binary = np.round(y_train).astype(int)  # Binary labels\n",
    "    y_val_binary = np.round(y_val).astype(int)\n",
    "\n",
    "    # Shuffle the resampled training data\n",
    "    X_train, y_train_binary = shuffle(X_train, y_train_binary, random_state=42)\n",
    "    \n",
    "    # Reshape the validation data to match CNN input\n",
    "    X_train = X_train.reshape((X_train.shape[0], 48, 48, 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], 48, 48, 1))\n",
    "\n",
    "    # Create CNN feature extractor\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor()\n",
    "    cnn_features_train = cnn_feature_extractor.predict(X_train)\n",
    "    cnn_features_val = cnn_feature_extractor.predict(X_val)\n",
    "    \n",
    "    # Flatten the CNN features\n",
    "    cnn_features_train = cnn_features_train.reshape((cnn_features_train.shape[0], -1))  # Flatten the features\n",
    "    cnn_features_val = cnn_features_val.reshape((cnn_features_val.shape[0], -1))  # Flatten the features\n",
    "    \n",
    "    # Create the DNN model\n",
    "    dnn_model = create_dnn(cnn_features_train.shape[1:])\n",
    "    \n",
    "    # Compile the DNN model\n",
    "    dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',     # Metric to monitor\n",
    "        patience=5,             # Number of epochs to wait for improvement\n",
    "        restore_best_weights=True  # Restore weights from the best epoch\n",
    "    )\n",
    "\n",
    "    # Ensure that y_train_binary and y_val_binary are reshaped to (batch_size, 1)\n",
    "    y_train_binary = y_train_binary.reshape((-1, 1))\n",
    "    y_val_binary = y_val_binary.reshape((-1, 1))\n",
    "\n",
    "    print(\"Shape of y_train_binary after reshaping:\", y_train_binary.shape)\n",
    "    print(\"Shape of y_val_binary after reshaping:\", y_val_binary.shape)\n",
    "\n",
    "    # Train the DNN model\n",
    "    history = dnn_model.fit(\n",
    "        cnn_features_train, y_train_binary,\n",
    "        validation_data=(cnn_features_val, y_val_binary),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Append the training history for later analysis\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the DNN model\n",
    "    val_preds = dnn_model.predict(cnn_features_val)\n",
    "    val_preds = np.round(val_preds).astype(int)  # Convert probabilities to binary 0 or 1\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    f1 = f1_score(y_val_binary, val_preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1 Score for Fold {fold}: {f1}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val_binary, val_preds)\n",
    "    print(f\"Confusion Matrix for Fold {fold}:\\n\", conf_matrix)\n",
    "\n",
    "    # Increment fold number\n",
    "    fold += 1\n",
    "\n",
    "# Print average F1 score\n",
    "print(f\"\\nAverage F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy')\n",
    "plt.title('Model Accuracy Across Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['loss'], label=f'Fold {i+1} Train Loss')\n",
    "    plt.plot(history['val_loss'], label=f'Fold {i+1} Validation Loss')\n",
    "plt.title('Model Loss Across Folds')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
