{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "X_labeled = np.load(\"Xtrain1.npy\")\n",
    "X_unlabeled = np.load(\"Xtrain1_extra.npy\")\n",
    "Y_train_labeled = np.load(\"Ytrain1.npy\")\n",
    "X_test = np.load(\"Xtest1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_labeled.shape)\n",
    "print(X_unlabeled.shape)\n",
    "\n",
    "#Normalization\n",
    "\n",
    "X_labeled = X_labeled/255\n",
    "X_unlabeled = X_unlabeled/255\n",
    "X_test = X_test/255\n",
    "\n",
    "resized = np.resize(X_labeled[1], [48,48])\n",
    "plt.imshow(resized, cmap='gray')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Imbalance\n",
    "\n",
    "crater_images = sum(Y_train_labeled)\n",
    "no_crater_images = len(Y_train_labeled) - crater_images\n",
    "ratio_no_crater_crater = no_crater_images/crater_images\n",
    "print(\"Number of crater images: \", crater_images)\n",
    "print(\"Number of no crater images: \", no_crater_images)\n",
    "print(\"Ratio no crater/crater: \", ratio_no_crater_crater)\n",
    "\n",
    "#Class Weights\n",
    "\n",
    "class_weights_first = compute_class_weight(class_weight='balanced', classes=np.unique(Y_train_labeled), y=Y_train_labeled.flatten())\n",
    "class_weight_dict_first = dict(enumerate(class_weights_first))\n",
    "print(\"Class weights: \", class_weight_dict_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and Data Splitting\n",
    "\n",
    "X_train_reshaped = X_labeled.reshape(len(X_labeled), 48, 48, 1)\n",
    "X_extra_train = X_unlabeled.reshape(len(X_unlabeled), 48, 48, 1)\n",
    "X_test_reshaped = X_test.reshape(len(X_test), 48, 48, 1)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_reshaped, Y_train_labeled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for Data Imbalance: Data Augmentation only on Minority Class\n",
    "\n",
    "Data augmentation is a technique that can be used to improve the robustness and accuracy of a machine learning model by creating additional examples from the original dataset.\n",
    "\n",
    "https://www.picsellia.com/post/improve-imbalanced-datasets-in-computer-vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_indices = np.where(Y_train == 1)[0]\n",
    "minority_indices = np.where(Y_train == 0)[0]\n",
    "\n",
    "X_train_majority = X_train[majority_indices]\n",
    "X_train_minority = X_train[minority_indices]\n",
    "\n",
    "target_minority_samples = len(X_train_majority)\n",
    "num_augmented_samples = target_minority_samples - len(X_train_minority)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "augmented_minority_images = []\n",
    "for i in range(num_augmented_samples):\n",
    "    img = X_train_minority[i % len(X_train_minority)].reshape((1, 48, 48, 1))\n",
    "    augmented_img = next(datagen.flow(img, batch_size=1))[0]\n",
    "    augmented_minority_images.append(augmented_img)\n",
    "\n",
    "X_train_minority_balanced = np.concatenate([X_train_minority, np.array(augmented_minority_images)])\n",
    "Y_train_minority_balanced = np.zeros(len(X_train_minority_balanced))\n",
    "\n",
    "X_train_balanced_DA = np.concatenate([X_train_majority, X_train_minority_balanced])\n",
    "Y_train_balanced_DA = np.concatenate([np.ones(len(X_train_majority)), Y_train_minority_balanced])\n",
    "\n",
    "indices = np.arange(len(Y_train_balanced_DA))\n",
    "np.random.shuffle(indices)\n",
    "X_train_balanced_DA, Y_train_balanced_DA = X_train_balanced_DA[indices], Y_train_balanced_DA[indices]\n",
    "\n",
    "print(\"Balanced training set shape:\", X_train_balanced_DA.shape)\n",
    "print(\"Balanced class distribution:\", np.bincount(Y_train_balanced_DA.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for Data Imbalance: Random Sampling\n",
    "\n",
    "https://medium.com/@shubhamgupta.3101994/addressing-data-imbalance-in-image-classification-techniques-and-strategies-b922fb3c5124\n",
    "https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "\n",
    "There are two main approaches to random resampling for imbalanced classification; they are oversampling and undersampling.\n",
    "\n",
    " - Random Oversampling: Randomly duplicate examples in the minority class.\n",
    " - Random Undersampling: Randomly delete examples in the majority class.\n",
    "\n",
    "Random resampling provides a naive technique for rebalancing the class distribution for an imbalanced dataset.\n",
    "Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\n",
    "\n",
    "Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Over Sampling (ROS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
    "ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_ROS, Y_train_ROS = ros.fit_resample(X_train_flattened, Y_train)\n",
    "X_train_ROS = X_train_ROS.reshape(-1, 48, 48, 1)\n",
    "unique, counts = np.unique(Y_train_ROS, return_counts=True)\n",
    "print(\"Balanced class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Under Sampling (RUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_RUS, Y_train_RUS = rus.fit_resample(X_train_flattened, Y_train)\n",
    "X_train_RUS = X_train_RUS.reshape(-1, 48, 48, 1)\n",
    "unique, counts = np.unique(Y_train_RUS, return_counts=True)\n",
    "print(\"Balanced class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_SMOTE, Y_train_SMOTE = smote.fit_resample(X_train_flattened, Y_train)\n",
    "X_train_SMOTE = X_train_SMOTE.reshape(-1, 48, 48, 1)\n",
    "unique, counts = np.unique(Y_train_SMOTE, return_counts=True)\n",
    "print(\"Balanced class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of ROS and RUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
    "ros = RandomOverSampler(sampling_strategy=0.95, random_state=42)\n",
    "X_train_over, Y_train_over = ros.fit_resample(X_train_flattened, Y_train)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "X_train_both, Y_train_both = rus.fit_resample(X_train_over, Y_train_over)\n",
    "\n",
    "X_train_both = X_train_both.reshape(-1, 48, 48, 1)\n",
    "\n",
    "unique, counts = np.unique(Y_train_both, return_counts=True)\n",
    "print(\"Final balanced class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional Layer 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flattening\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layer with dropout\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer (binary classification)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    adam = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer= adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and summarize the model\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # Metric to monitor\n",
    "    patience=5, # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_both, Y_train_both,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_preds = model.predict(X_val)\n",
    "val_preds = np.round(val_preds)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(Y_val, val_preds))\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_val, val_preds)\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_val, val_preds)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plotting training history\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rgb = np.repeat(X_train_both, 3, axis=-1)  # Converts (48, 48, 1) to (48, 48, 3)\n",
    "X_val_rgb = np.repeat(X_val, 3, axis=-1)\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(48, 48, 3)\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train_rgb, Y_train_both,\n",
    "    validation_data=(X_val_rgb, Y_val),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "val_preds = model.predict(X_val_rgb)\n",
    "val_preds = np.round(val_preds)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(Y_val, val_preds))\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_val, val_preds)\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_val, val_preds)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plotting training history\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the unlabeled data (values between 0 and 1)\n",
    "X_unlabeled_norm = X_unlabeled / 255.0\n",
    "X_unlabeled_norm = X_unlabeled_norm.reshape(X_unlabeled_norm.shape[0], 48, 48, 1)\n",
    "\n",
    "# Train the CNN model on labeled data first (the code you already have)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Train the model on the labeled dataset\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Step 1: Use the trained CNN model to generate pseudo-labels for the unlabeled data\n",
    "# Get predictions for the unlabeled data\n",
    "unlabeled_preds = model.predict(X_unlabeled_norm)\n",
    "\n",
    "# Step 2: Apply a threshold to assign pseudo-labels based on prediction confidence\n",
    "# For binary classification, pseudo-labels are 0 or 1 depending on confidence\n",
    "pseudo_labels = np.where(unlabeled_preds > 0.9, 1, np.where(unlabeled_preds < 0.1, 0, -1))\n",
    "\n",
    "# Only keep confident predictions (those with pseudo-labels not equal to -1)\n",
    "confident_indices = np.where(pseudo_labels != -1)[0]\n",
    "X_confident = X_unlabeled_norm[confident_indices]\n",
    "pseudo_labels_confident = pseudo_labels[confident_indices]\n",
    "# Reshape pseudo_labels_confident if needed\n",
    "pseudo_labels_confident = pseudo_labels_confident.ravel()\n",
    "\n",
    "\n",
    "# Step 3: Combine the original labeled data with the pseudo-labeled confident data\n",
    "X_combined = np.concatenate((X_train, X_confident), axis=0)\n",
    "y_combined = np.concatenate((y_train, pseudo_labels_confident), axis=0)\n",
    "\n",
    "print(X_combined.shape)\n",
    "print(X_combined)\n",
    "\n",
    "# Step 4: Retrain the model using the combined labeled + pseudo-labeled data\n",
    "model = create_model()  # Recreate the model to retrain\n",
    "history_combined = model.fit(\n",
    "    datagen.flow(X_combined, y_combined, batch_size=batch_size),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "val_preds = model.predict(X_val)\n",
    "val_preds = np.round(val_preds)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_val, val_preds))\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_val, val_preds)\n",
    "print(\"F1 Score after self-training: \", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "print(\"Confusion Matrix after self-training:\\n\", conf_matrix)\n",
    "\n",
    "# Step 6: Plotting training history for combined data\n",
    "plt.plot(history_combined.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history_combined.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy After Self-Training')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_combined.history['loss'], label='train loss')\n",
    "plt.plot(history_combined.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss After Self-Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "f1_scores = []\n",
    "histories = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_labeled_norm):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X_labeled_norm[train_index], X_labeled_norm[val_index]\n",
    "    y_train, y_val = Y_train[train_index], Y_train[val_index]\n",
    "    \n",
    "    # Flatten the training data for RandomOverSampling\n",
    "    X_train_flattened = X_train.reshape((X_train.shape[0], -1))\n",
    "    \n",
    "    # Apply RandomOverSampling to balance the training data\n",
    "    ros = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_flattened, y_train)\n",
    "    \n",
    "    # Reshape the resampled data back to image shape\n",
    "    X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], 48, 48, 1))\n",
    "    \n",
    "    # Shuffle the resampled training data\n",
    "    X_train_resampled, y_train_resampled = shuffle(X_train_resampled, y_train_resampled, random_state=42)\n",
    "    \n",
    "    # Create a new CNN model for each fold\n",
    "    model = create_model()\n",
    "\n",
    "    # Train the model with data augmentation\n",
    "    history = model.fit(\n",
    "        datagen.flow(X_train_resampled, y_train_resampled, batch_size=32), #if data augmentation is before data split\n",
    "        validation_data=(X_val.reshape(X_val.shape[0], 48, 48, 1), y_val),  # Ensure X_val has shape (48, 48, 1)\n",
    "        epochs=20\n",
    "    )\n",
    "    \n",
    "    # Append the training history for later analysis\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_preds = model.predict(X_val.reshape(X_val.shape[0], 48, 48, 1))\n",
    "    val_preds = np.round(val_preds)  # Convert probabilities to binary 0 or 1\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    f1 = f1_score(y_val, val_preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1 Score for Fold {fold}: {f1}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "    print(f\"Confusion Matrix for Fold {fold}:\\n\", conf_matrix)\n",
    "\n",
    "    # Increment fold number\n",
    "    fold += 1\n",
    "\n",
    "# Print average F1 score\n",
    "print(f\"\\nAverage F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy')\n",
    "plt.title('Model Accuracy Across Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['loss'], label=f'Fold {i+1} Train Loss')\n",
    "    plt.plot(history['val_loss'], label=f'Fold {i+1} Validation Loss')\n",
    "plt.title('Model Loss Across Folds')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CV and the new training set (labeled +  labeled through self-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "f1_scores = []\n",
    "histories = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_combined):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X_combined[train_index], X_combined[val_index]\n",
    "    y_train, y_val = y_combined[train_index], y_combined[val_index]\n",
    "    \n",
    "    # Flatten the training data for RandomOverSampling\n",
    "    X_train_flattened = X_train.reshape((X_train.shape[0], -1))\n",
    "    \n",
    "    # Apply RandomOverSampling to balance the training data\n",
    "    ros = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_flattened, y_train)\n",
    "    \n",
    "    # Reshape the resampled data back to image shape\n",
    "    X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], 48, 48, 1))\n",
    "    \n",
    "    # Shuffle the resampled training data\n",
    "    X_train_resampled, y_train_resampled = shuffle(X_train_resampled, y_train_resampled, random_state=42)\n",
    "    \n",
    "    # Create a new CNN model for each fold\n",
    "    model = create_model()\n",
    "\n",
    "    # Train the model with data augmentation\n",
    "    history = model.fit(\n",
    "        datagen.flow(X_train_resampled, y_train_resampled, batch_size=32), #if data augmentation is before data split\n",
    "        validation_data=(X_val.reshape(X_val.shape[0], 48, 48, 1), y_val),  # Ensure X_val has shape (48, 48, 1)\n",
    "        epochs=20\n",
    "    )\n",
    "    \n",
    "    # Append the training history for later analysis\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_preds = model.predict(X_val.reshape(X_val.shape[0], 48, 48, 1))\n",
    "    val_preds = np.round(val_preds)  # Convert probabilities to binary 0 or 1\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    f1 = f1_score(y_val, val_preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1 Score for Fold {fold}: {f1}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "    print(f\"Confusion Matrix for Fold {fold}:\\n\", conf_matrix)\n",
    "\n",
    "    # Increment fold number\n",
    "    fold += 1\n",
    "\n",
    "# Print average F1 score\n",
    "print(f\"\\nAverage F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy')\n",
    "plt.title('Model Accuracy Across Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['loss'], label=f'Fold {i+1} Train Loss')\n",
    "    plt.plot(history['val_loss'], label=f'Fold {i+1} Validation Loss')\n",
    "plt.title('Model Loss Across Folds')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a CNN for feature extraction and training the model with a DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (replace with actual loading code)\n",
    "X = np.load('Xtrain1.npy')  \n",
    "y = np.load('Ytrain1.npy')  \n",
    "\n",
    "# Preprocessing: Normalize images (pixel values between 0 and 1)\n",
    "X = X / 255.0\n",
    "\n",
    "# Reshape the data to add channel dimension (grayscale images)\n",
    "X = X.reshape(X.shape[0], 48, 48, 1)\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "f1_scores = []\n",
    "histories = []\n",
    "\n",
    "# Data augmentation using ImageDataGenerator for CNN training\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "# Build the CNN feature extractor model\n",
    "def create_cnn_feature_extractor():\n",
    "    inputs = Input(shape=(48, 48, 1))\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Flattening\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return Model(inputs, x)\n",
    "\n",
    "# Build the DNN model\n",
    "def create_dnn(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Flatten the training data for RandomOverSampling\n",
    "    X_train_flattened = X_train.reshape((X_train.shape[0], -1))\n",
    "    \n",
    "    # Apply RandomOverSampling to balance the training data\n",
    "    ros = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_flattened, y_train)\n",
    "    \n",
    "    # Reshape the resampled data back to image shape\n",
    "    X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], 48, 48, 1))\n",
    "    \n",
    "    # Shuffle the resampled training data\n",
    "    X_train_resampled, y_train_resampled = shuffle(X_train_resampled, y_train_resampled, random_state=42)\n",
    "    \n",
    "    # Create CNN feature extractor\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor()\n",
    "    cnn_features_train = cnn_feature_extractor.predict(X_train_resampled)\n",
    "    cnn_features_val = cnn_feature_extractor.predict(X_val.reshape(X_val.shape[0], 48, 48, 1))\n",
    "    \n",
    "    # Ensure that the features are flattened before feeding to DNN\n",
    "    cnn_features_train = cnn_features_train.reshape((cnn_features_train.shape[0], -1))  # Flatten the features\n",
    "    cnn_features_val = cnn_features_val.reshape((cnn_features_val.shape[0], -1))  # Flatten the features\n",
    "    \n",
    "    # Create the DNN model\n",
    "    dnn_model = create_dnn(cnn_features_train.shape[1:])\n",
    "    \n",
    "    # Compile the DNN model\n",
    "    dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the DNN model without data augmentation\n",
    "    history = dnn_model.fit(\n",
    "        cnn_features_train, y_train_resampled,\n",
    "        validation_data=(cnn_features_val, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Append the training history for later analysis\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the DNN model\n",
    "    val_preds = dnn_model.predict(cnn_features_val)\n",
    "    val_preds = np.round(val_preds)  # Convert probabilities to binary 0 or 1\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    f1 = f1_score(y_val, val_preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1 Score for Fold {fold}: {f1}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "    print(f\"Confusion Matrix for Fold {fold}:\\n\", conf_matrix)\n",
    "\n",
    "    # Increment fold number\n",
    "    fold += 1\n",
    "\n",
    "# Print average F1 score\n",
    "print(f\"\\nAverage F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy')\n",
    "plt.title('Model Accuracy Across Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['loss'], label=f'Fold {i+1} Train Loss')\n",
    "    plt.plot(history['val_loss'], label=f'Fold {i+1} Validation Loss')\n",
    "plt.title('Model Loss Across Folds')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (replace with actual loading code)\n",
    "X = X_combined\n",
    "y = y_combined\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "f1_scores = []\n",
    "histories = []\n",
    "\n",
    "# Data augmentation using ImageDataGenerator for CNN training\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "# Build the CNN feature extractor model\n",
    "def create_cnn_feature_extractor():\n",
    "    inputs = Input(shape=(48, 48, 1))\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Flattening\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return Model(inputs, x)\n",
    "\n",
    "# Build the DNN model\n",
    "def create_dnn(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Flatten the training data for RandomOverSampling\n",
    "    X_train_flattened = X_train.reshape((X_train.shape[0], -1))\n",
    "    \n",
    "    # Apply RandomOverSampling to balance the training data\n",
    "    ros = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_flattened, y_train)\n",
    "    \n",
    "    # Reshape the resampled data back to image shape\n",
    "    X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], 48, 48, 1))\n",
    "    \n",
    "    # Shuffle the resampled training data\n",
    "    X_train_resampled, y_train_resampled = shuffle(X_train_resampled, y_train_resampled, random_state=42)\n",
    "    \n",
    "    # Create CNN feature extractor\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor()\n",
    "    cnn_features_train = cnn_feature_extractor.predict(X_train_resampled)\n",
    "    cnn_features_val = cnn_feature_extractor.predict(X_val.reshape(X_val.shape[0], 48, 48, 1))\n",
    "    \n",
    "    # Ensure that the features are flattened before feeding to DNN\n",
    "    cnn_features_train = cnn_features_train.reshape((cnn_features_train.shape[0], -1))  # Flatten the features\n",
    "    cnn_features_val = cnn_features_val.reshape((cnn_features_val.shape[0], -1))  # Flatten the features\n",
    "    \n",
    "    # Create the DNN model\n",
    "    dnn_model = create_dnn(cnn_features_train.shape[1:])\n",
    "    \n",
    "    # Compile the DNN model\n",
    "    dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the DNN model without data augmentation\n",
    "    history = dnn_model.fit(\n",
    "        cnn_features_train, y_train_resampled,\n",
    "        validation_data=(cnn_features_val, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Append the training history for later analysis\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the DNN model\n",
    "    val_preds = dnn_model.predict(cnn_features_val)\n",
    "    val_preds = np.round(val_preds)  # Convert probabilities to binary 0 or 1\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    f1 = f1_score(y_val, val_preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1 Score for Fold {fold}: {f1}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "    print(f\"Confusion Matrix for Fold {fold}:\\n\", conf_matrix)\n",
    "\n",
    "    # Increment fold number\n",
    "    fold += 1\n",
    "\n",
    "# Print average F1 score\n",
    "print(f\"\\nAverage F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['accuracy'], label=f'Fold {i+1} Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label=f'Fold {i+1} Validation Accuracy')\n",
    "plt.title('Model Accuracy Across Folds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history['loss'], label=f'Fold {i+1} Train Loss')\n",
    "    plt.plot(history['val_loss'], label=f'Fold {i+1} Validation Loss')\n",
    "plt.title('Model Loss Across Folds')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
